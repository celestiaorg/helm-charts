# Copyright Broadcom, Inc. All Rights Reserved.
# SPDX-License-Identifier: APACHE-2.0

## @section Global parameters
## Global Docker image parameters
## Please, note that this will override the image parameters, including dependencies, configured to use the global value
## Current available global Docker image parameters: imageRegistry, imagePullSecrets and storageClass
##

## @param global.imageRegistry Global Docker image registry
## @param global.imagePullSecrets Global Docker registry secret names as an array
## @param global.storageClass Global StorageClass for Persistent Volume(s)
##
global:
  imageRegistry: ""
  ## e.g:
  ## imagePullSecrets:
  ##   - myRegistryKeySecretName
  ##
  imagePullSecrets: []
  storageClass: ""
  ## Compatibility adaptations for Kubernetes platforms
  ##
  compatibility:
    ## Compatibility adaptations for Openshift
    ##
    openshift:
      ## @param global.compatibility.openshift.adaptSecurityContext Adapt the securityContext sections of the deployment to make them compatible with Openshift restricted-v2 SCC: remove runAsUser, runAsGroup and fsGroup and let the platform use their allowed default IDs. Possible values: auto (apply if the detected running cluster is Openshift), force (perform the adaptation always), disabled (do not perform adaptation)
      ##
      adaptSecurityContext: auto

## @section Common parameters
##

## @param kubeVersion Override Kubernetes version
##
kubeVersion: ""
## @param nameOverride String to partially override common.names.name
##
nameOverride: ""
## @param fullnameOverride String to fully override common.names.fullname
##
fullnameOverride: ""
## @param namespaceOverride String to fully override common.names.namespace
##
namespaceOverride: ""
## @param commonLabels Labels to add to all deployed objects
##
commonLabels: {}
## @param commonAnnotations Annotations to add to all deployed objects
##
commonAnnotations: {}
## @param clusterDomain Kubernetes cluster domain name
##
clusterDomain: cluster.local
## @param extraDeploy Array of extra objects to deploy with the release
##
extraDeploy: []
## Diagnostic mode
## @param diagnosticMode.enabled Enable diagnostic mode (all probes will be disabled and the command will be overridden)
## @param diagnosticMode.command Command to override all containers in the chart release
## @param diagnosticMode.args Args to override all containers in the chart release
##
diagnosticMode:
  enabled: false
  command:
    - sleep
  args:
    - infinity

## @section celestia-node Parameters
##

## %%MAIN_CONTAINER/POD_DESCRIPTION%%
##
node:
  ## Celestia celestia-node image
  ## ref: https://github.com/celestiaorg/celestia-node/pkgs/container/celestia-node/versions?filters%5Bversion_type%5D=tagged
  ## @param node.image.registry [default: REGISTRY_NAME] celestia-node image registry
  ## @param node.image.repository [default: REPOSITORY_NAME/celestia-node] celestia-node image repository
  ## @skip node.image.tag celestia-node image tag (immutable tags are recommended)
  ## @param node.image.digest celestia-node image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag image tag (immutable tags are recommended)
  ## @param node.image.pullPolicy celestia-node image pull policy
  ## @param node.image.pullSecrets celestia-node image pull secrets
  ##
  # -- image parameters for the image
  image:
    # -- registry for the image, GitHub Container Registry by default
    registry: ghcr.io
    # -- repository for the image, celestiaorg/celestia-node by default
    repository: celestiaorg/celestia-node
    # -- tag for the image, v0.13.6 by default
    tag: v0.14.0
    digest: ""
    ## Specify a imagePullPolicy
    ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
    ## ref: https://kubernetes.io/docs/concepts/containers/images/#pre-pulled-images
    ##
    # -- pull policy for the image, IfNotPresent by default
    pullPolicy: IfNotPresent
    ## Optionally specify an array of imagePullSecrets.
    ## Secrets must be manually created in the namespace.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ## e.g:
    ## pullSecrets:
    ##   - myRegistryKeySecretName
    ##
    pullSecrets: []
  ## @param node.settings Settings for the celestia-node
  settings:
    ## Select from: bridge, full, light
    ## Valid values:
    ## - bridge
    ## - full
    ## - light:
    nodeType: bridge
    ## @param node.settings.home Home directory for the celestia-node
    ##
    # -- home directory for the celestia-node, defaults to /home/celestia
    home: "/home/celestia"
    ## @param node.settings.node_id Node ID for the celestia-node
    ##
    # -- node ID for the celestia-node, it must be set
    node_id: "SET_IT"
    ## @param node.settings.address Address for the celestia-node
    ##
    # -- address for the celestia-node, it must be set
    address: "SET_IT"
    ## @param node.settings.secret Secret for the celestia-node
    ##
    ## A secret must be available in the cluster with the following format:
    ## - name: <secret-name>
    ## - items:
    ##     - key: NJ3XILLTMVRXEZLUFZVHO5A
    ##     - key: OAZHALLLMV4Q
    ##     - key: my_celes_key_info
    secret:
      # -- name of the secret, it must be set
      name: SET_IT
  ## @param node.config Configuration for the celestia-node
  ##
  ## @param node.config.apptoml Configuration for the celestia-node node.toml
  ##
  ## @param node.config.clienttoml Configuration for the celestia-node client.toml
  ##
  ## @param node.config.configtoml Configuration for the celestia-node config.toml
  ##
  config:
    configtoml:
      Node:
        StartupTimeout: 2m0s
        ShutdownTimeout: 2m0s
      Core:
        IP: ""
        RPCPort: "26657"
        GRPCPort: "9090"
      State:
        KeyringAccName: ""
        KeyringBackend: test
        GranterAddress: []
      P2P:
        ListenAddresses:
          # IPv4
          - /ip4/0.0.0.0/udp/2121/quic-v1/webtransport
          - /ip4/0.0.0.0/udp/2121/quic-v1
          #- /ip4/0.0.0.0/tcp/2122/wss
          - /ip4/0.0.0.0/tcp/2121
          # IPv6
          - /ip6/::/udp/2121/quic-v1/webtransport
          - /ip6/::/udp/2121/quic-v1
          #- /ip6/::/tcp/2122/wss
          - /ip6/::/tcp/2121
        AnnounceAddresses: []
        NoAnnounceAddresses:
          # IPv4
          - /ip4/127.0.0.1/udp/2121/quic-v1/webtransport
          - /ip4/127.0.0.1/udp/2121/quic-v1
          - /ip4/127.0.0.1/tcp/2122/wss
          - /ip4/127.0.0.1/tcp/2121
          # IPv6
          - /ip6/::1/udp/2121/quic-v1/webtransport
          - /ip6/::1/udp/2121/quic-v1
          - /ip6/::1/tcp/2122/wss
          - /ip6/::1/tcp/2121
        MutualPeers: []
        PeerExchange: true
        RoutingTableRefreshPeriod: 1m0s
        ConnManager:
          Low: 800
          High: 1000
          GracePeriod: 1m0s
      RPC:
        Address: localhost
        Port: "26658"
        SkipAuth: false
      Gateway:
        Address: localhost
        Port: "26659"
        Enabled: false
      Share:
        UseShareExchange: true
        EDSStoreParams:
          GCInterval: 0s
          RecentBlocksCacheSize: 10
          BlockstoreCacheSize: 128
        ShrExEDSParams:
          ServerReadTimeout: 5s
          ServerWriteTimeout: 1m0s
          HandleRequestTimeout: 1m0s
          ConcurrencyLimit: 10
          BufferSize: 32768
        ShrExNDParams:
          ServerReadTimeout: 5s
          ServerWriteTimeout: 1m0s
          HandleRequestTimeout: 1m0s
          ConcurrencyLimit: 10
        PeerManagerParams:
          PoolValidationTimeout: 2m0s
          PeerCooldown: 3s
          GcInterval: 30s
          EnableBlackListing: false
        Discovery:
          PeersLimit: 5
          AdvertiseInterval: 1h0m0s
      Header:
        TrustedHash: ""
        TrustedPeers: []
        Store:
          StoreCacheSize: 4096
          IndexCacheSize: 16384
          WriteBatchSize: 2048
        Syncer:
          TrustingPeriod: 336h0m0s
        Server:
          WriteDeadline: 8s
          ReadDeadline: 1m0s
          RangeRequestTimeout: 10s
        Client:
          MaxHeadersPerRangeRequest: 64
          RangeRequestTimeout: 8s
      Pruner:
        EnableService: false
  ## @param node.otelAgent OTel agent configuration for the node
  ##
  otelAgent:
    # -- enable otel agent for the node, false by default
    enabled: false
    # -- image for the otel agent, ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib:0.102.0 by default
    image:
      registry: ghcr.io
      repository: open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib
      tag: 0.102.0
      digest: ""
      pullPolicy: IfNotPresent
    ## Init container's resource requests and limits
    ## ref: http://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ## @param volumePermissions.resourcesPreset Set init container resources according to one common preset (allowed values: none, nano, small, medium, large, xlarge, 2xlarge). This is ignored if volumePermissions.resources is set (volumePermissions.resources is recommended for production).
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    ##
    #resourcesPreset: "micro"
    ## @param volumePermissions.resources Set init container resources for the otel agent (essential for production workloads)
    ## Example:
    resources:
      requests:
        cpu: 100m
        memory: 100Mi
      #limits:
        #cpu: 200m
        #memory: 200Mi
    ## A secret must be available in the cluster with the following format:
    ## - name: <secret-name>
    ## - items:
    ##     - key: token
    ##     - key: username
    # -- grafana otel secret for the node
    grafanaOtelSecret:
      # -- name of the grafana otel secret, it must be set
      name: SET_IT
    ## @param node.otelAgent.config Configuration for the otel agent
    ##
    # -- config for the otel agent (See: https://opentelemetry.io/docs/collector/configuration/)
    config:
      extensions:
        basicauth/otlp:
          client_auth:
            username: "${GRAFANA_OTEL_USERNAME}"
            password: "${GRAFANA_OTEL_TOKEN}"
      receivers:
        otlp:
          protocols:
            grpc:
              endpoint: "localhost:4317"
            http:
              endpoint: "localhost:4318"
        prometheus:
          config:
            scrape_configs:
              - job_name: "${JOB_NAME}"
                scrape_interval: 10s
                static_configs:
                  - targets: ["localhost:8890"]
      exporters:
        otlphttp:
          auth:
            authenticator: basicauth/otlp
          endpoint: https://otlp-gateway-prod-us-central-0.grafana.net/otlp
        prometheus:
          endpoint: "localhost:8889"
      service:
        extensions: [basicauth/otlp]
        pipelines:
          metrics:
            receivers: [otlp, prometheus]
            exporters: [otlphttp, prometheus]
        telemetry:
          metrics:
            address: "localhost:8888"
            level: basic
          logs:
            level: INFO
  ## @param node.replicaCount Number of node replicas to deploy
  ##
  # -- number of node replicas to deploy, 1 by default
  replicaCount: 1
  ## @param node.containerPorts.p2p Node P2P container port
  ## @param node.containerPorts.rest Node REST container port
  ## @param node.containerPorts.rpc Node RPC container port
  ## @param node.containerPorts.profiling Node Profiling container port
  ## @param node.containerPorts.prometheus Node Prometheus container port
  ##
  # -- Container ports for the node
  containerPorts:
    # -- WebSocket container port, 2122 by default
    ws: 2122
    # -- P2P container port, 2121 by default
    p2p: 2121
    # -- REST container port, 26659 by default
    rest: 26659
    # -- RPC container port, 26658 by default
    rpc: 26658
    # -- Profiling container port, 6060 by default
    profiling: 6060
    # -- Prometheus container port, 8890 by default
    prometheus: 8890
  ## Configure extra options for node containers' liveness and readiness probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  ## @param node.livenessProbe.enabled Enable livenessProbe on node containers
  ## @param node.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
  ## @param node.livenessProbe.periodSeconds Period seconds for livenessProbe
  ## @param node.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
  ## @param node.livenessProbe.failureThreshold Failure threshold for livenessProbe
  ## @param node.livenessProbe.successThreshold Success threshold for livenessProbe
  ##
  # -- liveness probe for the node
  livenessProbe:
    # -- enable liveness probe on node containers, false by default
    enabled: false
    # -- initial delay seconds for livenessProbe, 0 by default
    initialDelaySeconds: 0
    # -- period seconds for livenessProbe, 10 by default
    periodSeconds: 10
    # -- timeout seconds for livenessProbe, 1 by default
    timeoutSeconds: 1
    # -- failure threshold for livenessProbe, 3 by default
    failureThreshold: 3
    # -- success threshold for livenessProbe, 1 by default
    successThreshold: 1
  ## @param node.readinessProbe.enabled Enable readinessProbe on node containers
  ## @param node.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
  ## @param node.readinessProbe.periodSeconds Period seconds for readinessProbe
  ## @param node.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
  ## @param node.readinessProbe.failureThreshold Failure threshold for readinessProbe
  ## @param node.readinessProbe.successThreshold Success threshold for readinessProbe
  ##
  readinessProbe:
    # -- enable readiness probe on node containers, false by default
    enabled: true
    # -- initial delay seconds for readinessProbe, 0 by default
    initialDelaySeconds: 0
    # -- period seconds for readinessProbe, 10 by default
    periodSeconds: 10
    # -- timeout seconds for readinessProbe, 1 by default
    timeoutSeconds: 1
    # -- failure threshold for readinessProbe, 3 by default
    failureThreshold: 3
    # -- success threshold for readinessProbe, 1 by default
    successThreshold: 1
  ## @param node.startupProbe.enabled Enable startupProbe on node containers
  ## @param node.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
  ## @param node.startupProbe.periodSeconds Period seconds for startupProbe
  ## @param node.startupProbe.timeoutSeconds Timeout seconds for startupProbe
  ## @param node.startupProbe.failureThreshold Failure threshold for startupProbe
  ## @param node.startupProbe.successThreshold Success threshold for startupProbe
  ##
  # -- startup probe for the node
  startupProbe:
    # -- enable startup probe on node containers, false by default
    enabled: false
    # -- initial delay seconds for startupProbe, 0 by default
    initialDelaySeconds: 0
    # -- period seconds for startupProbe, 10 by default
    periodSeconds: 10
    # -- timeout seconds for startupProbe, 1 by default
    timeoutSeconds: 1
    # -- failure threshold for startupProbe, 3 by default
    successThreshold: 1
  ## @param node.customLivenessProbe Custom livenessProbe that overrides the default one
  ##
  customLivenessProbe: {}
  ## @param node.customReadinessProbe Custom readinessProbe that overrides the default one
  ##
  customReadinessProbe: {}
  ## @param node.customStartupProbe Custom startupProbe that overrides the default one
  ##
  customStartupProbe: {}
  ## node resource requests and limits
  ## ref: http://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
  ## @param node.resourcesPreset Set node container resources according to one common preset (allowed values: none, nano, small, medium, large, xlarge, 2xlarge). This is ignored if node.resources is set (node.resources is recommended for production).
  ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
  ##
  # -- set node container resources according to one common preset (allowed values: none, nano, small, medium, large, xlarge, 2xlarge). This is ignored if node.resources is set (node.resources is recommended for production)
  # -- more information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
  resourcesPreset: "nano"
  ## @param node.resources Set node container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Example:
  ## resources:
  ##   requests:
  ##     cpu: 2
  ##     memory: 512Mi
  ##   limits:
  ##     cpu: 3
  ##     memory: 1024Mi
  ##
  # -- resources for the node
  resources:
    # -- requests for the node
    requests:
      # -- cpu requests for the node, 2 by default
      cpu: 6
      # -- memory requests for the node, 8Gi by default
      memory: 16Gi
    # -- limits for the node
    limits:
      # -- cpu limits for the node, 2 by default
      #cpu: 6
      # -- memory limits for the node, 8Gi by default
      memory: 16Gi
  ## Configure Pods Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param node.podSecurityContext.enabled Enable node pods' Security Context
  ## @param node.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy for node pods
  ## @param node.podSecurityContext.sysctls Set kernel settings using the sysctl interface for node pods
  ## @param node.podSecurityContext.supplementalGroups Set filesystem extra groups for node pods
  ## @param node.podSecurityContext.fsGroup Set fsGroup in node pods' Security Context
  ##
  # -- pod security context for the node
  podSecurityContext:
    # -- enable pod security context for the node, true by default
    enabled: true
    # -- filesystem group change policy for node pods, Always by default
    fsGroupChangePolicy: Always
    # -- kernel settings using the sysctl interface for node pods, [] by default
    sysctls: []
    # -- filesystem extra groups for node pods, [] by default
    supplementalGroups: []
    # -- fsGroup in node pods' Security Context, 10001 by default
    fsGroup: 10001
  ## Configure Container Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
  ## @param node.containerSecurityContext.enabled Enabled node container' Security Context
  ## @param node.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in node container
  ## @param node.containerSecurityContext.runAsUser Set runAsUser in node container' Security Context
  ## @param node.containerSecurityContext.runAsNonRoot Set runAsNonRoot in node container' Security Context
  ## @param node.containerSecurityContext.readOnlyRootFilesystem Set readOnlyRootFilesystem in node container' Security Context
  ## @param node.containerSecurityContext.privileged Set privileged in node container' Security Context
  ## @param node.containerSecurityContext.allowPrivilegeEscalation Set allowPrivilegeEscalation in node container' Security Context
  ## @param node.containerSecurityContext.capabilities.drop List of capabilities to be dropped in node container
  ## @param node.containerSecurityContext.seccompProfile.type Set seccomp profile in node container
  ##
  # -- container security context for the node
  containerSecurityContext:
    # -- enable container security context for the node, true by default
    enabled: true
    # -- SELinux options in node container, {} by default
    seLinuxOptions: {}
    # -- runAsUser in node container, 10001 by default
    runAsUser: 10001
    # -- runAsGroup in node container, 10001 by default
    runAsGroup: 10001
    # -- runAsNonRoot in node container, true by default
    runAsNonRoot: true
    # -- readOnlyRootFilesystem in node container, true by default
    readOnlyRootFilesystem: true
    # -- privileged in node container, false by default
    privileged: false
    # -- allowPrivilegeEscalation in node container, false by default
    allowPrivilegeEscalation: false
    # -- capabilities to be dropped in node container, ["ALL"] by default
    capabilities:
      drop: ["ALL"]
    # -- seccomp profile in node container, RuntimeDefault by default
    seccompProfile:
      type: "RuntimeDefault"

  ## @param node.existingConfigmap The name of an existing ConfigMap with your custom configuration for node
  ##
  existingConfigmap:
  ## @param node.command Override default node container command (useful when using custom images)
  ##
  # -- command for the celestia-node
  command:
    # -- celestia
    - celestia
  ## @param node.args Override default node container args (useful when using custom images)
  ##
  args:
    - bridge
    - start
    - --node.store=$(CELESTIA_HOME)
    - --metrics
    - --metrics.tls=false
    - --p2p.metrics
  ## @param node.automountServiceAccountToken Mount Service Account token in node pods
  ##
  # -- mount service account token in node pods
  automountServiceAccountToken: false
  ## @param node.hostAliases node pods host aliases
  ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
  ##
  hostAliases: []
  ## @param node.daemonsetAnnotations Annotations for node daemonset
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  daemonsetAnnotations: {}
  ## @param node.deploymentAnnotations Annotations for node deployment
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  deploymentAnnotations: {}
  ## @param node.statefulsetAnnotations Annotations for node statefulset
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  statefulsetAnnotations: {}
  ## @param node.podLabels Extra labels for node pods
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
  ##
  podLabels: {}
  ## @param node.podAnnotations Annotations for node pods
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  podAnnotations: {}
  ## @param node.podAffinityPreset Pod affinity preset. Ignored if `node.affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAffinityPreset: ""
  ## @param node.podAntiAffinityPreset Pod anti-affinity preset. Ignored if `node.affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAntiAffinityPreset: soft
  ## Node node.affinity preset
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  ##
  nodeAffinityPreset:
    ## @param node.nodeAffinityPreset.type Node affinity preset type. Ignored if `node.affinity` is set. Allowed values: `soft` or `hard`
    ##
    type: ""
    ## @param node.nodeAffinityPreset.key Node label key to match. Ignored if `node.affinity` is set
    ##
    key: ""
    ## @param node.nodeAffinityPreset.values Node label values to match. Ignored if `node.affinity` is set
    ## E.g.
    ## values:
    ##   - e2e-az1
    ##   - e2e-az2
    ##
    values: []
  ## @param node.affinity Affinity for node pods assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## NOTE: `node.podAffinityPreset`, `node.podAntiAffinityPreset`, and `node.nodeAffinityPreset` will be ignored when it's set
  ##
  affinity: {}
  ## @param node.nodeSelector Node labels for node pods assignment
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
  ##
  nodeSelector: {}
  ## @param node.tolerations Tolerations for node pods assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []
  ## ONLY FOR DEPLOYMENTS:
  ## @param node.updateStrategy.type node deployment strategy type
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
  ## ONLY FOR STATEFULSETS:
  ## @param node.updateStrategy.type node statefulset strategy type
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
  ##
  updateStrategy:
    ## ONLY FOR DEPLOYMENTS:
    ## Can be set to RollingUpdate or Recreate
    ## ONLY FOR STATEFULSETS:
    ## Can be set to RollingUpdate or OnDelete
    ##
    type: RollingUpdate
  ## ONLY FOR STATEFULSETS:
  ## @param node.persistentVolumeClaimRetentionPolicy Persistent Volume Claim Retention Policy for the node
  ## ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#retain
  ##
  persistentVolumeClaimRetentionPolicy:
    ## @param node.persistentVolumeClaimRetentionPolicy.whenDeleted Policy when the StatefulSet is deleted. Allowed values: Retain, Delete
    ##
    whenDeleted: Retain
    ## @param node.persistentVolumeClaimRetentionPolicy.whenScaled Policy when the StatefulSet is scaled down. Allowed values: Retain, Delete
    ##
    whenScaled: Retain
  ## ONLY FOR STATEFULSETS:
  ## @param node.podManagementPolicy Pod management policy for node statefulset
  ## Ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies
  ##
  podManagementPolicy: OrderedReady
  ## @param node.priorityClassName node pods' priorityClassName
  ##
  priorityClassName: ""
  ## @param node.topologySpreadConstraints Topology Spread Constraints for node pod assignment spread across your cluster among failure-domains
  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
  ##
  topologySpreadConstraints: []
  ## @param node.schedulerName Name of the k8s scheduler (other than default) for node pods
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  schedulerName: ""
  ## @param node.terminationGracePeriodSeconds Seconds node pods need to terminate gracefully
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods
  ##
  terminationGracePeriodSeconds: ""
  ## @param node.lifecycleHooks for node containers to automate configuration before or after startup
  ##
  lifecycleHooks: {}
  ## @param node.extraEnvVars Array with extra environment variables to add to node containers
  ## e.g:
  ## extraEnvVars:
  ##   - name: FOO
  ##     value: "bar"
  ##
  extraEnvVars: []
  ## @param node.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for node containers
  ##
  extraEnvVarsCM: ""
  ## @param node.extraEnvVarsSecret Name of existing Secret containing extra env vars for node containers
  ##
  extraEnvVarsSecret: ""
  ## @param node.extraVolumes Optionally specify extra list of additional volumes for the node pods
  ##
  extraVolumes: []
  ## @param node.extraVolumeMounts Optionally specify extra list of additional volumeMounts for the node containers
  ##
  extraVolumeMounts: []
  ## @param node.sidecars Add additional sidecar containers to the node pods
  ## e.g:
  ## sidecars:
  ##   - name: your-image-name
  ##     image: your-image
  ##     imagePullPolicy: Always
  ##     ports:
  ##       - name: portname
  ##         containerPort: 1234
  ##
  sidecars: []
  ## @param node.initContainers Add additional init containers to the node pods
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
  ## e.g:
  ## initContainers:
  ##  - name: your-image-name
  ##    image: your-image
  ##    imagePullPolicy: Always
  ##    command: ['sh', '-c', 'echo "hello world"']
  ##
  initContainers: []
  ## Pod Disruption Budget configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
  ## @param node.pdb.create Enable/disable a Pod Disruption Budget creation
  ## @param node.pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
  ## @param node.pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable
  ##
  pdb:
    create: false
    minAvailable: 1
    maxUnavailable: ""

  # TODO: Remove when not needed in the end
  # TODO: If you add here something remember to add it to:_helpers.tpl (define "node.imagePullSecrets") and NOTES.txt ({{- include "common.warnings.rollingTag" .Values.%%OTHER_OBJECT_BLOCK%%.image }})
# ## %%SECONDARY_CONTAINER/POD_DESCRIPTION%%
# ##
# %%SECONDARY_OBJECT_BLOCK%%:
#   %%SAME_STRUCTURE_AS_THE_MAIN_CONTAINER/POD%%

# ## %%OTHERS_CONTAINER/POD_DESCRIPTION%%
# ##
# %%OTHER_OBJECT_BLOCK%%:
#   %%SAME_STRUCTURE_AS_THE_MAIN_CONTAINER/POD%%

## @section Traffic Exposure Parameters
##

## node service parameters
##
# -- service parameters
service:
  ## @param service.internal.type node internal service type
  ##
  # -- service type, ClusterIP by default
  internal:
    # -- service type, ClusterIP by default
    type: ClusterIP
    ## @param service.internal.ports.p2p Node internal service P2P port
    ## @param service.internal.ports.rest Node internal service REST port
    ## @param service.internal.ports.rpc Node internal service RPC port
    ## @param service.internal.ports.profiling Node internal service Profiling port
    ## @param service.internal.ports.prometheus Node internal service Prometheus port
    # -- Ports for the celestia-node
    ports:
      # -- WebSocket container port, 2122 by default
      ws: 2122
      # -- P2P container port, 2121 by default
      p2p: 2121
      # -- REST container port, 26659 by default
      rest: 26659
      # -- RPC container port, 26658 by default
      rpc: 26658
      # -- Profiling container port, 6060 by default
      profiling: 6060
      # -- Prometheus container port, 8890 by default
      prometheus: 8890
    ## @param service.internal.clusterIP node internal service Cluster IP
    ## e.g.:
    ## clusterIP: None # We use headless mode by default for stateful workloads.
    ##
    clusterIP: "None"
    ## @param service.internal.annotations Additional custom annotations for node internal service
    ##
    annotations: {}
    ## @param service.internal.sessionAffinity Control where client requests go, to the same pod or round-robin
    ## Values: ClientIP or None
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
    ##
    sessionAffinity: None
    ## @param service.internal.sessionAffinityConfig Additional settings for the sessionAffinity
    ## sessionAffinityConfig:
    ##   clientIP:
    ##     timeoutSeconds: 300
    ##
    sessionAffinityConfig: {}

  ## @param service.external.enabled Enable external service
  ##
  # -- external service parameters
  external:
    # -- enable external service, true by default
    enabled: true
    ## @param service.external.type node external service type
    ##
    # -- external service type, ClusterIP by default
    type: ClusterIP
    ## @param service.external.ports.p2p Node external service P2P port
    ## @param service.external.ports.rest Node external service REST port
    ## @param service.external.ports.rpc Node external service RPC port
    ## @param service.external.ports.profiling Node external service Profiling port
    ## @param service.external.ports.prometheus Node external service Prometheus port
    ports:
      # -- P2P container port, 2122 by default
      ws: 2122
      # -- P2P container port, 2121 by default
      p2p: 2121
      # -- REST container port, 26659 by default
      rest: 26659
      # -- RPC container port, 26658 by default
      rpc: 26658
    ## Node ports to expose
    ## @param service.external.nodePorts.p2p Node port for P2P
    ## @param service.external.nodePorts.rest Node port for REST
    ## @param service.external.nodePorts.rpc Node port for RPC
    ## @param service.external.nodePorts.profiling Node port for Profiling
    ## @param service.external.nodePorts.prometheus Node port for Prometheus
    ## NOTE: choose port between <30000-32767>
    ##
    # -- node ports for the celestia-app
    nodePorts:
      # -- p2p port, 2122 by default
      ws: ""
      # -- p2p port, 2121 by default
      p2p: ""
      # -- rest port, 26659 by default
      rest: ""
      # -- rpc port, 26658 by default
      rpc: ""
      # -- profiling port, 6060 by default
      profiling: ""
      # -- prometheus port, 8890 by default
      prometheus: ""
    ## @param service.external.loadBalancerIP node external service Load Balancer IP
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
    ##
    loadBalancerIP: ""
    ## @param service.internal.clusterIP node internal service Cluster IP
    ## e.g.:
    ## clusterIP: None # We use headless mode by default for stateful workloads.
    ##
    clusterIP: "None"
    ## @param service.internal.annotations Additional custom annotations for node internal service
    ##
    ## @param service.external.loadBalancerSourceRanges node external service Load Balancer sources
    ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
    ## e.g:
    ## loadBalancerSourceRanges:
    ##   - 10.10.10.0/24
    ##
    loadBalancerSourceRanges: []
    ## @param service.external.externalTrafficPolicy node external service external traffic policy
    ## ref http://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
    ##
    externalTrafficPolicy: Cluster
    ## @param service.external.annotations Additional custom annotations for node external service
    ##
    annotations: {}
    ## @param service.external.extraPorts Extra ports to expose in node external service (normally used with the `sidecars` value)
    ##
    extraPorts: []
    ## @param service.external.sessionAffinity Control where client requests go, to the same pod or round-robin
    ## Values: ClientIP or None
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
    ##
    sessionAffinity: None
    ## @param service.external.sessionAffinityConfig Additional settings for the sessionAffinity
    ## sessionAffinityConfig:
    ##   clientIP:
    ##     timeoutSeconds: 300
    ##
    sessionAffinityConfig: {}
## Network Policies
## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
##
# -- network policy, The Policy model to apply. When set to false, only pods with the correct
# -- server label will have network access to the ports server is listening
# -- on. When true, server will accept connections from any source
# -- (with the correct destination port).
networkPolicy:
  ## @param networkPolicy.enabled Specifies whether a NetworkPolicy should be created
  ##
  # -- enable network policy, true by default
  enabled: true
  ## @param networkPolicy.allowExternal Don't require server label for connections
  ## The Policy model to apply. When set to false, only pods with the correct
  ## server label will have network access to the ports server is listening
  ## on. When true, server will accept connections from any source
  ## (with the correct destination port).
  ##
  allowExternal: true
  ## @param networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
  ##
  allowExternalEgress: true
  ## @param networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
  ## e.g:
  ## extraIngress:
  ##   - ports:
  ##       - port: 1234
  ##     from:
  ##       - podSelector:
  ##           - matchLabels:
  ##               - role: frontend
  ##       - podSelector:
  ##           - matchExpressions:
  ##               - key: role
  ##                 operator: In
  ##                 values:
  ##                   - frontend
  extraIngress: []
  ## @param networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy (ignored if allowExternalEgress=true)
  ## e.g:
  ## extraEgress:
  ##   - ports:
  ##       - port: 1234
  ##     to:
  ##       - podSelector:
  ##           - matchLabels:
  ##               - role: frontend
  ##       - podSelector:
  ##           - matchExpressions:
  ##               - key: role
  ##                 operator: In
  ##                 values:
  ##                   - frontend
  ##
  extraEgress: []
  ## @param networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces
  ## @param networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces
  ##
  ingressNSMatchLabels: {}
  ingressNSPodMatchLabels: {}

## @section Persistence Parameters
##

## Enable persistence using Persistent Volume Claims
## ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/
##
# -- persistence parameters
persistence:
  ## @param persistence.enabled Enable persistence using Persistent Volume Claims
  ##
  # -- enable persistence, true by default
  enabled: true
  ## @param persistence.mountPath Path to mount the volume at.
  ## Note: This value is overridden by 'Values.node.settings.home' in the stateful set.
  ##
  mountPath: /bitnami/app/data
  ## @param persistence.subPath The subdirectory of the volume to mount to, useful in dev environments and one PV for multiple services
  ## Note: This is not used in the stateufulset template
  ##
  subPath: ""
  ## @param persistence.storageClass Storage class of backing PVC
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS & OpenStack)
  ##
  storageClass: ""
  ## @param persistence.annotations Persistent Volume Claim annotations
  ##
  annotations: {}
  ## @param persistence.accessModes Persistent Volume Access Modes
  ##
  accessModes:
    - ReadWriteOnce
  ## @param persistence.size Size of data volume
  ##
  # -- size of data volume, 250Gi by default
  size: 250Gi
  ## @param persistence.existingClaim The name of an existing PVC to use for persistence
  ##
  existingClaim: ""
  ## @param persistence.selector Selector to match an existing Persistent Volume for WordPress data PVC
  ## If set, the PVC can't have a PV dynamically provisioned for it
  ## E.g.
  ## selector:
  ##   matchLabels:
  ##     app: my-app
  ##
  selector: {}
  ## @param persistence.dataSource Custom PVC data source
  ##
  dataSource: {}
## @section Init Container Parameters
##

## 'volumePermissions' init container parameters
## Changes the owner and group of the persistent volume mount point to runAsUser:fsGroup values
##   based on the *podSecurityContext/*containerSecurityContext parameters
##
volumePermissions:
  ## @param volumePermissions.enabled Enable init container that changes the owner/group of the PV mount point to `runAsUser:fsGroup`
  ##
  enabled: false
  ## OS Shell + Utility image
  ## ref: https://hub.docker.com/r/bitnami/os-shell/tags/
  ## @param volumePermissions.image.registry [default: REGISTRY_NAME] OS Shell + Utility image registry
  ## @param volumePermissions.image.repository [default: REPOSITORY_NAME/os-shell] OS Shell + Utility image repository
  ## @skip volumePermissions.image.tag OS Shell + Utility image tag (immutable tags are recommended)
  ## @param volumePermissions.image.pullPolicy OS Shell + Utility image pull policy
  ## @param volumePermissions.image.pullSecrets OS Shell + Utility image pull secrets
  ##
  image:
    registry: docker.io
    repository: bitnami/os-shell
    tag: 12-debian-12-r22
    pullPolicy: IfNotPresent
    ## Optionally specify an array of imagePullSecrets.
    ## Secrets must be manually created in the namespace.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ## e.g:
    ## pullSecrets:
    ##   - myRegistryKeySecretName
    ##
    pullSecrets: []
  ## Init container's resource requests and limits
  ## ref: http://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
  ## @param volumePermissions.resourcesPreset Set init container resources according to one common preset (allowed values: none, nano, small, medium, large, xlarge, 2xlarge). This is ignored if volumePermissions.resources is set (volumePermissions.resources is recommended for production).
  ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
  ##
  resourcesPreset: "nano"
  ## @param volumePermissions.resources Set init container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Example:
  ## resources:
  ##   requests:
  ##     cpu: 2
  ##     memory: 512Mi
  ##   limits:
  ##     cpu: 3
  ##     memory: 1024Mi
  ##
  resources: {}
  ## Init container Container Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
  ## @param volumePermissions.containerSecurityContext.enabled Enabled init container' Security Context
  ## @param volumePermissions.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in init container
  ## @param volumePermissions.containerSecurityContext.runAsUser Set init container's Security Context runAsUser
  ## NOTE: when runAsUser is set to special value "auto", init container will try to chown the
  ##   data folder to auto-determined user&group, using commands: `id -u`:`id -G | cut -d" " -fc2`
  ##   "auto" is especially useful for OpenShift which has scc with dynamic user ids (and 0 is not allowed)
  ##
  containerSecurityContext:
    enabled: true
    seLinuxOptions: {}
    runAsUser: 0

## @section Other Parameters
##

## RBAC configuration
##
rbac:
  ## @param rbac.create Specifies whether RBAC resources should be created
  ##
  create: false
  ## @param rbac.rules Custom RBAC rules to set
  ## e.g:
  ## rules:
  ##   - apiGroups:
  ##       - ""
  ##     resources:
  ##       - pods
  ##     verbs:
  ##       - get
  ##       - list
  ##
  rules: []

## ServiceAccount configuration
##
serviceAccount:
  ## @param serviceAccount.create Specifies whether a ServiceAccount should be created
  ##
  create: true
  ## @param serviceAccount.name The name of the ServiceAccount to use.
  ## If not set and create is true, a name is generated using the common.names.fullname template
  ##
  name: ""
  ## @param serviceAccount.annotations Additional Service Account annotations (evaluated as a template)
  ##
  annotations: {}
  ## @param serviceAccount.automountServiceAccountToken Automount service account token for the server service account
  ##
  automountServiceAccountToken: true

## Prometheus metrics
##
metrics:
  ## @param metrics.enabled Enable the export of Prometheus metrics
  ##
  enabled: false
  ## Prometheus Operator ServiceMonitor configuration
  ##
  serviceMonitor:
    ## @param metrics.serviceMonitor.enabled if `true`, creates a Prometheus Operator ServiceMonitor (also requires `metrics.enabled` to be `true`)
    ##
    enabled: false
    ## @param metrics.serviceMonitor.namespace Namespace in which Prometheus is running
    ##
    namespace: ""
    ## @param metrics.serviceMonitor.annotations Additional custom annotations for the ServiceMonitor
    ##
    annotations: {}
    ## @param metrics.serviceMonitor.labels Extra labels for the ServiceMonitor
    ##
    labels: {}
    ## @param metrics.serviceMonitor.jobLabel The name of the label on the target service to use as the job name in Prometheus
    ##
    jobLabel: ""
    ## @param metrics.serviceMonitor.honorLabels honorLabels chooses the metric's labels on collisions with target labels
    ##
    honorLabels: false
    ## @param metrics.serviceMonitor.interval Interval at which metrics should be scraped.
    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
    ## e.g:
    ## interval: 10s
    ##
    interval: ""
    ## @param metrics.serviceMonitor.scrapeTimeout Timeout after which the scrape is ended
    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
    ## e.g:
    ## scrapeTimeout: 10s
    ##
    scrapeTimeout: ""
    ## @param metrics.serviceMonitor.metricRelabelings Specify additional relabeling of metrics
    ##
    metricRelabelings: []
    ## @param metrics.serviceMonitor.relabelings Specify general relabeling
    ##
    relabelings: []
    ## @param metrics.serviceMonitor.selector Prometheus instance selector labels
    ## ref: https://github.com/bitnami/charts/tree/main/bitnami/prometheus-operator#prometheus-configuration
    ## selector:
    ##   prometheus: my-prometheus
    ##
    selector: {}
